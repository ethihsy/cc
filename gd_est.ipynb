{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hideOutput": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "Bernoulli = tf.contrib.distributions.Bernoulli\n",
    "\n",
    "data = input_data.read_data_sets(\"MNIST_data/\", one_hot=True).train\n",
    "data.images[data.images>0.5] = 1.0\n",
    "data.images[data.images<=0.5] = 0.0\n",
    "#data.images[:,0:392] = (data.images[:,0:392]-.5)*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "for i in range(14):\n",
    "    for j in range(14):\n",
    "        data.images[:,i*14+j] = data.images[:,56*i+2*j] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def plot_banded_error(errs, label, ax=None):\n",
    "    ax = ax or plt\n",
    "    if np.shape(errs)[0]==2:\n",
    "        s = np.mean(np.std(errs,1), axis=0)\n",
    "        line, = ax.plot(np.arange(1, len(s)+1), np.square(s), label=label, linewidth=3)\n",
    "    else:\n",
    "        m = np.mean(errs, axis=0)\n",
    "    #    m = np.mean(np.reshape(m,[-1,10]),1)\n",
    "        s = np.std(errs, axis=0)\n",
    "        line, = ax.plot(np.arange(1, len(m)+1), m, label=label, linewidth=3)\n",
    "    #    ax.fill_between(np.arange(1, len(m)+1), m-s, m+s, color=line.get_color(), alpha=0.075)\n",
    "\n",
    "def fatlegend(ax, *a, **kw):\n",
    "    leg = ax.legend(*a, **kw)\n",
    "    for l in leg.legendHandles:\n",
    "        l.set_linewidth(l.get_linewidth()*2.0)\n",
    "    return leg\n",
    "\n",
    "def jacobian(y_flat, x):\n",
    "    n = y_flat.shape[0]\n",
    "    loop_vars = [\n",
    "        tf.constant(0, tf.int32),\n",
    "        tf.TensorArray(tf.float32, size=n),\n",
    "    ]\n",
    "    _, jacobian = tf.while_loop(\n",
    "        lambda j, _: j < n,\n",
    "        lambda j, result: (j+1, result.write(j, tf.gradients(y_flat[j], x))),\n",
    "        loop_vars)\n",
    "    return tf.squeeze(jacobian.stack())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def get_weights(wn):\n",
    "    with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "        return tf.get_variable(wn)\n",
    "\n",
    "def add_layer(inputs, wn, in_dim, out_dim, af=None):\n",
    "    try:\n",
    "        w = tf.get_variable(wn, [out_dim, in_dim+1], initializer=tf.truncated_normal_initializer)\n",
    "    except:\n",
    "        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
    "            w = tf.get_variable(wn)\n",
    "    inputs = tf.concat([inputs, tf.ones([tf.shape(inputs)[0],1])], 1)\n",
    "    ww = tf.transpose(w)\n",
    "    return tf.matmul(inputs, ww) if af is None else af(tf.matmul(inputs, ww))\n",
    "\n",
    "\n",
    "likelihood_ratio = lambda f, p, s, w: opt.compute_gradients(tf.reduce_mean(\n",
    "                                          tf.stop_gradient(f) * log_prob(tf.tile(p,[ns,1]),s), 0), w)\n",
    "prob = lambda p, s: (tf.pow(tf.clip_by_value(p,eps,1), s)\n",
    "                     * tf.pow(1-tf.clip_by_value(p,0,1-eps), 1-s))\n",
    "log_prob = lambda p, s: tf.log(prob(p,s))\n",
    "\n",
    "ts  = tf.placeholder(tf.float32)\n",
    "lr  = tf.placeholder(tf.float32)\n",
    "tau = tf.placeholder(tf.float32)\n",
    "\n",
    "eps = 1e-7\n",
    "dim = 392\n",
    "#dim = 98\n",
    "nn = 200\n",
    "ns = tf.placeholder(tf.int32)\n",
    "\n",
    "y_ = tf.placeholder(tf.float32, [None, dim])\n",
    "x  = tf.placeholder(tf.float32, [None, dim])\n",
    "\n",
    "h   = add_layer(x, \"wxh\", dim, nn, tf.nn.sigmoid)\n",
    "sh  = tf.stop_gradient(tf.reshape(Bernoulli(probs=h, dtype=tf.float32).sample(ns),[-1,nn]))\n",
    "h2  = add_layer(sh, \"whh\", nn, nn, tf.nn.sigmoid)\n",
    "sh2 = tf.stop_gradient(tf.reshape(Bernoulli(probs=h2, dtype=tf.float32).sample(ns),[-1,nn]))\n",
    "y   = add_layer(sh2, \"why\", nn, dim, tf.nn.sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "yh  = -log_prob(y, tf.tile(y_,[ns**2,1]))\n",
    "nll = tf.reduce_mean(tf.reduce_sum(yh,1))\n",
    "yx  = tf.reduce_mean(tf.reshape(yh,[ns,-1,dim]),0)\n",
    "\n",
    "opt = tf.train.AdamOptimizer(lr)\n",
    "dy = opt.compute_gradients(nll, get_weights(\"why\")) \n",
    "dh = likelihood_ratio(tf.reduce_sum(yh,1,keep_dims=True), h2, sh2, get_weights(\"whh\"))\n",
    "dx = likelihood_ratio(tf.reduce_sum(yx,1,keep_dims=True), h, sh, get_weights(\"wxh\"))\n",
    "\n",
    "gd = [list(i) for i in dy]+[list(i) for i in dx]\n",
    "for i in range(2):\n",
    "    gd[i][0] = tf.check_numerics(gd[i][0], str(i))\n",
    "#gdn = (tf.norm(gd[2][0])+tf.norm(gd[1][0]))/2.\n",
    "train = opt.apply_gradients(gd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "def muProp(p, s, layer, wx, wh, wy):\n",
    "    p_ = tf.concat([p, tf.ones([tf.shape(p)[0],1])], 1)\n",
    "    if layer==1:\n",
    "        p_ = tf.nn.sigmoid(tf.matmul(p_,tf.transpose(wh)))\n",
    "        p_ = tf.concat([p_, tf.ones([tf.shape(p_)[0],1])], 1)\n",
    "    z = tf.nn.sigmoid(tf.matmul(p_,tf.transpose(wy)))\n",
    "    z = -log_prob(z, tf.tile(y_,[ns**(layer-1),1]))\n",
    "    z = tf.reduce_sum(z,1,keep_dims=True)\n",
    "    \n",
    "    dfh = tf.gradients(z,p)[0]\n",
    "    z, dfh, p = [tf.tile(i, [ns,1]) for i in [z, dfh, p]]\n",
    "    g = z + tf.reduce_sum(dfh*(s-p),1,keep_dims=True)\n",
    "    w = wx if layer==1 else wh\n",
    "    mg = tf.stop_gradient(dfh)*p    \n",
    "    dmg = tf.gradients(tf.reduce_mean(mg, 0), w)[0]\n",
    "    return g, dmg\n",
    "\n",
    "cvh = muProp(h2, sh2, 2, get_weights(\"wxh\"), get_weights(\"whh\"), get_weights(\"why\"))\n",
    "cvx = muProp(h, sh, 1, get_weights(\"wxh\"), get_weights(\"whh\"), get_weights(\"why\"))\n",
    "\n",
    "mdh = likelihood_ratio(tf.reduce_sum(yh,1,keep_dims=True)-cvh[0], h2, sh2, get_weights(\"whh\"))\n",
    "mdx = likelihood_ratio(tf.reduce_sum(yx,1,keep_dims=True)-cvx[0], h, sh, get_weights(\"wxh\"))\n",
    "\n",
    "mpgd = [list(i) for i in dy]+[list(i) for i in mdh]+[list(i) for i in mdx]\n",
    "mpgd[1][0] += cvh[1]\n",
    "mpgd[2][0] += cvx[1]\n",
    "for i in range(3):\n",
    "    mpgd[i][0] = tf.check_numerics(mpgd[i][0], str(i))\n",
    "    \n",
    "mpgdn = [tf.norm(mpgd[2][0]), tf.norm(mpgd[1][0])]\n",
    "mptrain = opt.apply_gradients(mpgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mpidb(z, w, n, f, out):\n",
    "    s1 = add_layer(z, w+\"_\", n, nn/2, tf.nn.tanh)\n",
    "    s2 = add_layer(s1, w, nn/2, out, tf.nn.tanh)\n",
    "    loss = tf.square(tf.stop_gradient(f)-s2)\n",
    "    return loss, s2\n",
    "\n",
    "def sProp(p, layer, wx, wh, wy):\n",
    "    if layer==1:\n",
    "        s = mpidb(p, \"widbh\", nn, sh2-h2, nn)[1]\n",
    "        p = tf.concat([p, tf.ones([tf.shape(p)[0],1])], 1)        \n",
    "        p = tf.nn.sigmoid(tf.matmul(p, tf.transpose(wh))) + s\n",
    "    p = tf.concat([p, tf.ones([tf.shape(p)[0],1])], 1)    \n",
    "    z = tf.nn.sigmoid(tf.matmul(p,tf.transpose(wy)))\n",
    "    z = -log_prob(z, tf.tile(y_,[ns**(layer-1),1]))\n",
    "    return tf.tile(z,[ns,1])\n",
    "\n",
    "bh = mpidb(sh, \"widbh\", nn, sh2-h2, nn)\n",
    "bx = mpidb(x, \"widbx\", dim, sh-h, nn)\n",
    "\n",
    "cvh = sProp(bh[1]+h2, 2, get_weights(\"wxh\"), get_weights(\"whh\"), get_weights(\"why\"))\n",
    "cvx = sProp(bx[1]+h, 1, get_weights(\"wxh\"), get_weights(\"whh\"), get_weights(\"why\"))\n",
    "\n",
    "spdh = likelihood_ratio(tf.reduce_sum(yh-cvh,1,keep_dims=True), h2, sh2, get_weights(\"whh\"))\n",
    "spdx = likelihood_ratio(tf.reduce_sum(yx-cvx,1,keep_dims=True), h, sh, get_weights(\"wxh\"))\n",
    "\n",
    "spgd = [list(i) for i in dy]+[list(i) for i in spdh]+[list(i) for i in spdx]\n",
    "for i in range(3):\n",
    "    spgd[i][0] = tf.check_numerics(spgd[i][0], str(i))\n",
    "spgdn = (tf.norm(spgd[2][0])+tf.norm(spgd[1][0]))/2.\n",
    "sptrain = opt.apply_gradients(spgd)\n",
    "\n",
    "optt = tf.train.AdamOptimizer(lr*10, beta2=.9)\n",
    "ibtrain = optt.minimize(bx[0]+bh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "baselineh = tf.placeholder(tf.float32)\n",
    "baselinex = tf.placeholder(tf.float32)\n",
    "\n",
    "def idb(z, w, n, f, out):\n",
    "    s1 = add_layer(z, w+\"_\", n, nn/2, tf.nn.tanh)\n",
    "    s2 = add_layer(s1, w, nn/2, out)\n",
    "    loss = tf.square(tf.stop_gradient(f)-s2)\n",
    "    return loss, s2\n",
    "\n",
    "bsh = tf.cond(ts<1, lambda: tf.reduce_mean(yh),\n",
    "                    lambda: .999*baselineh + .001*tf.reduce_mean(yh))\n",
    "bsx = tf.cond(ts<1, lambda: tf.reduce_mean(yx),\n",
    "                    lambda: .999*baselinex + .001*tf.reduce_mean(yx))\n",
    "\n",
    "bx = idb(x, \"widbx\", dim, yx-bsx, 1)\n",
    "bh = idb(sh, \"widbh\", nn, yh-bsh, 1)\n",
    "\n",
    "bdh = likelihood_ratio(tf.reduce_sum(yh-bsh-bh[1],1,keep_dims=True), h2, sh2, get_weights(\"whh\"))\n",
    "bdx = likelihood_ratio(tf.reduce_sum(yx-bsx-bx[1],1,keep_dims=True), h, sh, get_weights(\"wxh\"))\n",
    "\n",
    "bgd = [list(i) for i in dy]+[list(i) for i in bdh]+[list(i) for i in bdx]\n",
    "for i in range(3):\n",
    "    bgd[i][0] = tf.check_numerics(bgd[i][0], str(i))\n",
    "bgdn = (tf.norm(bgd[2][0])+tf.norm(bgd[1][0]))/2.\n",
    "btrain = opt.apply_gradients(bgd)\n",
    "\n",
    "optt = tf.train.AdamOptimizer(lr*10, beta2=.99999)\n",
    "ibtrain = optt.minimize(bx[0]+bh[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def sample_baseline(f, layer):\n",
    "    n = 0 if layer==1 else 1\n",
    "    sf = tf.reduce_sum(tf.reshape(f,[ns,-1,dim]),n, keep_dims=True)\n",
    "    sf = tf.reshape(tf.tile(sf,[1,ns,1]), [-1,dim])\n",
    "    return (sf-f)/(tf.cast(ns,tf.float32)-1)\n",
    "\n",
    "ayh = sample_baseline(yh,2)\n",
    "ayx = sample_baseline(yx,1)\n",
    "\n",
    "bdh = likelihood_ratio(tf.reduce_sum(yh-ayh,1,keep_dims=True), h2, sh2, get_weights(\"whh\"))\n",
    "bdx = likelihood_ratio(tf.reduce_sum(yx-ayx,1,keep_dims=True), h, sh, get_weights(\"wxh\"))\n",
    "\n",
    "bgd = [list(i) for i in dy]+[list(i) for i in bdh]+[list(i) for i in bdx]\n",
    "for i in range(3):\n",
    "    bgd[i][0] = tf.check_numerics(bgd[i][0], str(i))\n",
    "bgdn = (tf.norm(bgd[2][0])+tf.norm(bgd[1][0]))/2.\n",
    "btrain = opt.apply_gradients(bgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def straight_through(p):\n",
    "    s = tf.stop_gradient(tf.reshape(Bernoulli(probs=p,dtype=tf.float32).sample(ns),[-1,nn])-p)+p\n",
    "    p2 = add_layer(s, \"whh\", nn, nn, tf.nn.sigmoid)\n",
    "    s2 = tf.stop_gradient(tf.reshape(Bernoulli(probs=p2,dtype=tf.float32).sample(ns),[-1,nn])-p2)+p2\n",
    "    sy = -log_prob(add_layer(s2, \"why\", nn, dim, tf.nn.sigmoid), tf.tile(y_,[ns**2,1]))\n",
    "    return tf.reduce_mean(tf.reduce_sum(sy,1))\n",
    "\n",
    "sttrain = opt.minimize(straight_through(h))\n",
    "\n",
    "\n",
    "def Gumbel_dist(p):\n",
    "    g0 = -tf.log(-tf.log(tf.random_uniform(tf.shape(p))))\n",
    "    g1 = -tf.log(-tf.log(tf.random_uniform(tf.shape(p))))\n",
    "    u1 = tf.exp((tf.log(tf.clip_by_value(p,eps,1))+g1)/tau)\n",
    "    u0 = tf.exp((tf.log(1-tf.clip_by_value(p,0,1-eps))+g0)/tau)\n",
    "    return u1/(u0+u1+eps)\n",
    "\n",
    "def Gumbel(p):\n",
    "    s = Gumbel_dist(p)\n",
    "    p2 = add_layer(s, \"whh\", nn, nn, tf.nn.sigmoid)\n",
    "    s2 = Gumbel_dist(p2)\n",
    "    sy = -log_prob(add_layer(s2, \"why\", nn, dim, tf.nn.sigmoid), tf.tile(y_,[ns**2,1]))\n",
    "    return tf.reduce_mean(tf.reduce_sum(sy,1))\n",
    "\n",
    "gbtrain = opt.minimize(Gumbel(h))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "wxh = tf.placeholder(tf.float32, [2,nn,dim+1])\n",
    "whh = tf.placeholder(tf.float32, [2,nn,nn+1])\n",
    "why = tf.placeholder(tf.float32, [2,dim,nn+1])\n",
    "\n",
    "feedforward = lambda p,w: tf.nn.sigmoid(tf.matmul(tf.concat([p,tf.ones([tf.shape(p)[0],1])],1), tf.transpose(w)))\n",
    "def mlmc(wx,wh,wy):\n",
    "    lh  = feedforward(x, wx)\n",
    "    lh2 = feedforward(sh, wh)\n",
    "    ly  = feedforward(sh2, wy)\n",
    "    lyh = -log_prob(ly, tf.tile(y_,[ns**2,1]))\n",
    "    lyx = tf.reduce_mean(tf.reshape(lyh, [ns,-1,dim]),0)\n",
    "    \n",
    "    ldy = tf.gradients(tf.reduce_mean(tf.reduce_sum(lyh,1)),wy)[0]\n",
    "    ldh = tf.gradients(tf.stop_gradient(tf.reduce_sum(lyh,1,keep_dims=True))*log_prob(tf.tile(lh2,[ns,1]),sh2),wh)[0]\n",
    "    ldx = tf.gradients(tf.stop_gradient(tf.reduce_sum(lyx,1,keep_dims=True))*log_prob(tf.tile(lh,[ns,1]),sh),wx)[0]\n",
    "    return ldx, ldh, ldy\n",
    "\n",
    "ldx, ldh, ldy = mlmc(wxh[1],whh[1],why[1])\n",
    "\n",
    "lgd = [list(i) for i in dy] + [list(i) for i in dh] + [list(i) for i in dx]\n",
    "lgd[2][0] -= ldx-wxh[0]\n",
    "lgd[1][0] -= ldh-whh[0]\n",
    "lgd[0][0] -= ldy-why[0]\n",
    "for i in range(3):\n",
    "    lgd[i][0] = tf.check_numerics(lgd[i][0], str(i))\n",
    "lgdn = (tf.norm(lgd[2][0])+tf.norm(lgd[1][0]))/2.\n",
    "ltrain = opt.apply_gradients(lgd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true
   },
   "outputs": [],
   "source": [
    "wxh2 = tf.placeholder(tf.float32, [2,nn,dim+1])\n",
    "whh2 = tf.placeholder(tf.float32, [2,nn,nn+1])\n",
    "why2 = tf.placeholder(tf.float32, [2,dim,nn+1])\n",
    "\n",
    "ldx2, ldh2, ldy2 = mlmc(wxh2[1],whh2[1],why2[1])\n",
    "\n",
    "lgd2 = [list(i) for i in dy] + [list(i) for i in dh] + [list(i) for i in dx]\n",
    "lgd2[2][0] -= ldx2-wxh2[0] + ldx-wxh[0]\n",
    "lgd2[1][0] -= ldh2-whh2[0] + ldh-whh[0]\n",
    "lgd2[0][0] -= ldy2-why2[0] + ldy-why[0]\n",
    "for i in range(3):\n",
    "    lgd2[i][0] = tf.check_numerics(lgd2[i][0], str(i))\n",
    "lgd2n = (tf.norm(lgd2[2][0])+tf.norm(lgd2[1][0]))/2.\n",
    "l2train = opt.apply_gradients(lgd2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false
   },
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": true,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "def fit_model(steps, filename, _lr, _ns, _mlmc):\n",
    "    loss_rec = np.empty((1000, steps/1000))\n",
    "    gd_rec = np.empty((1000, steps/1000))\n",
    "    _tau = 2.\n",
    "    _bsh, _bsx = 0., 0.\n",
    "    \n",
    "    _wxh = np.zeros([2,nn,dim+1])\n",
    "    _whh = np.zeros([2,nn,nn+1])\n",
    "    _why = np.zeros([2,dim,nn+1])\n",
    "    _wxh2 = np.zeros([2,nn,dim+1])\n",
    "    _whh2 = np.zeros([2,nn,nn+1])\n",
    "    _why2 = np.zeros([2,dim,nn+1])    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())            \n",
    "        for i in range(steps):\n",
    "            if i%100==0 and _mlmc:\n",
    "                batch_   = data.next_batch(24, shuffle=True)[0]\n",
    "                batch_xs = batch_[:, 0:dim]\n",
    "                batch_ys = batch_[:, dim:2*dim]\n",
    "                res = sess.run([nll, train, gd],\n",
    "                                {x: batch_xs, y_: batch_ys, ns: _ns, lr: _lr, ts:i, tau:_tau,\n",
    "                               })\n",
    "                _wxh, _whh, _why = res[2][2], res[2][1], res[2][0]\n",
    "                _wxh2, _whh2, _why2 = res[2][2], res[2][1], res[2][0]\n",
    "                \n",
    "            elif i%100==50 and _mlmc:\n",
    "                batch_   = data.next_batch(12, shuffle=True)[0]\n",
    "                batch_xs = batch_[:, 0:dim]\n",
    "                batch_ys = batch_[:, dim:2*dim]\n",
    "                res = sess.run([nll, ltrain, lgd],\n",
    "                                {x: batch_xs, y_: batch_ys, ns: _ns, lr: _lr, ts:i, tau:_tau,\n",
    "                                 wxh:_wxh, whh:_whh, why:_why,                                 \n",
    "                               })\n",
    "                _wxh2, _whh2, _why2 = res[2][2], res[2][1], res[2][0]\n",
    "            \n",
    "            else:\n",
    "                batch_   = data.next_batch(1, shuffle=True)[0]\n",
    "                batch_xs = batch_[:, 0:dim]\n",
    "                batch_ys = batch_[:, dim:2*dim]\n",
    "                res = sess.run([nll,\n",
    "    #                            gdn, train],\n",
    "    #                            mpgdn, mptrain],\n",
    "    #                            bgdn, btrain],\n",
    "    #                            gbtrain],\n",
    "    #                            sttrain],\n",
    "                                ltrain],\n",
    "                                {x: batch_xs, y_: batch_ys, ns: _ns, lr: _lr, ts:i, tau:_tau,\n",
    "                                 wxh:_wxh, whh:_whh, why:_why,\n",
    "#                                 wxh2:_wxh2, whh2:_whh2, why2:_why2,\n",
    "    #                             baselineh: _bsh, baselinex: _bsx,\n",
    "                               })                \n",
    "    #            _bsh,_bsx = res[-1], res[-2]\n",
    "                \n",
    "            loss_rec[i%1000, i/1000], gd_rec[i%1000, i/1000] = res[0], res[1]\n",
    "            if (i+1)%1000==0:\n",
    "                print (i+1)/1000, np.mean(loss_rec[:, i/1000], 0),\n",
    "\n",
    "    np.save(filename, [loss_rec, gd_rec]) \n",
    "    return loss_rec, gd_rec, batch_xs, batch_ys, res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fit_model(steps, filename, _lr, _ns, _mlmc):\n",
    "    loss_rec = np.empty((1000, steps/1000))\n",
    "    gd_rec = np.empty((2, 1000, steps/1000))\n",
    "    \n",
    "    _tau = 2.\n",
    "    _bsh, _bsx = 0., 0.\n",
    "    \n",
    "    _wxh = np.zeros([2,nn,dim+1])\n",
    "    _whh = np.zeros([2,nn,nn+1])\n",
    "    _why = np.zeros([2,dim,nn+1])\n",
    "    _wxh2 = np.zeros([2,nn,dim+1])\n",
    "    _whh2 = np.zeros([2,nn,nn+1])\n",
    "    _why2 = np.zeros([2,dim,nn+1])    \n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())            \n",
    "        for i in range(steps):\n",
    "\n",
    "            batch_   = data.next_batch(24, shuffle=True)[0]\n",
    "            batch_xs = batch_[:, 0:dim]\n",
    "            batch_ys = batch_[:, dim:2*dim]\n",
    "            res = sess.run([nll,\n",
    "#                            gdn, train],\n",
    "                            mpgdn, mptrain],\n",
    "#                            spgdn, sptrain, ibtrain],\n",
    "#                            bgdn, btrain],\n",
    "#                            gbtrain],\n",
    "#                            sttrain],\n",
    "#                            ltrain],\n",
    "                            {x: batch_xs, y_: batch_ys, ns: _ns, lr: _lr, ts:i, tau:_tau,\n",
    "#                             wxh:_wxh, whh:_whh, why:_why,\n",
    "#                                 wxh2:_wxh2, whh2:_whh2, why2:_why2,\n",
    "#                             baselineh: _bsh, baselinex: _bsx,\n",
    "                           })                \n",
    "#            _bsh,_bsx = res[-1], res[-2]\n",
    "                \n",
    "            loss_rec[i%1000, i/1000] = res[0]\n",
    "            gd_rec[:, i%1000, i/1000] = res[1]\n",
    "            \n",
    "            if (i+1)%1000==0:\n",
    "                print (i+1)/1000, np.mean(loss_rec[:, i/1000], 0),\n",
    "\n",
    "    np.save(filename, [loss_rec, gd_rec]) \n",
    "    return loss_rec, gd_rec, batch_xs, batch_ys, res[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false
   },
   "outputs": [],
   "source": [
    "mp0 = fit_model(1000000, \"mp_3e-4\", 3e-4, 1, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('default')\n",
    "fig, ax = plt.subplots(1,2, figsize=(16,5))\n",
    "ax[0].set_ylabel(\"Test negative log-likelihood\")\n",
    "ax[1].set_ylabel(\"Graidnet norm\")\n",
    "\n",
    "#ax[0].set_ylim(70,100)\n",
    "#ax[0].set_ylim(20,40)\n",
    "#ax[0].set_xlim(0,50)\n",
    "ax[1].set_ylim(0,100)\n",
    "for i in range(2):\n",
    "#    ax[i].set_xlabel(\"Steps with the same # grads (1e4)\")\n",
    "#    ax[i].set_xlabel(\"Steps (1e4)\")\n",
    "    plot_banded_error(test[i], \"mp\", ax[i])\n",
    "    \n",
    "    \n",
    "    \n",
    "    fatlegend(ax[0])\n",
    "    ax[i].grid(True)\n",
    "plt.show()\n",
    "#fig.savefig('nll2_2.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "batch_xs = ngd_svrg[2]\n",
    "batch_ys = ngd_svrg[3]\n",
    "prdt = ngd_svrg[4]\n",
    "bs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "hideCode": false,
    "hidePrompt": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,2, figsize=(16,8))\n",
    "up = np.concatenate([j for j in [np.reshape(i,[7,14]) for i in batch_xs]], 1)\n",
    "dw = np.concatenate([j for j in [np.reshape(i,[7,14]) for i in batch_ys]], 1)\n",
    "p1 = np.concatenate(np.split(np.concatenate([up,dw], 0), 1, 1), 0)\n",
    "#dwy = np.concatenate([j for j in [np.reshape(i,[14,28]) for i in prdt]], 1)\n",
    "#p2 = np.concatenate(np.split(np.concatenate([up,dwy], 0), bs, 1), 0)\n",
    "ax[0].imshow(p1, cmap=plt.cm.gray, interpolation='none')\n",
    "#ax[1].imshow(p2, cmap=plt.cm.gray, interpolation='none')\n",
    "ax[0].grid(False)\n",
    "#ax[1].grid(False)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
